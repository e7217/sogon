#!/usr/bin/env python3
"""
YouTube ë§í¬ì—ì„œ ìŒì„±ì„ ì¶”ì¶œí•˜ê³  Groq Whisper Turboë¡œ ìë§‰ ìƒì„±
API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ GROQ_API_KEYì— ì„¤ì •í•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”.
"""

import os
import tempfile
import re
from pathlib import Path
from datetime import datetime
from groq import Groq
import yt_dlp
from pydub import AudioSegment

def download_youtube_audio(url, output_dir=None):
    """
    YouTube ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œ
    
    Args:
        url (str): YouTube URL
        output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì—†ìœ¼ë©´ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
    
    Returns:
        str: ë‹¤ìš´ë¡œë“œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    """
    if output_dir is None:
        output_dir = tempfile.mkdtemp()
    
    # yt-dlp ì˜µì…˜ ì„¤ì •
    ydl_opts = {
        'format': 'bestaudio/best',
        'extractaudio': True,
        'audioformat': 'wav',
        'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
            'preferredquality': '192',
        }],
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            info = ydl.extract_info(url, download=False)
            title = info.get('title', 'unknown')
            
            # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            safe_title = re.sub(r'[<>:"/\\|?*]', '', title)
            output_path = os.path.join(output_dir, f"{safe_title}.wav")
            
            # ë‹¤ìš´ë¡œë“œ
            ydl.download([url])
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì°¾ê¸°
            for file in os.listdir(output_dir):
                if file.endswith('.wav'):
                    return os.path.join(output_dir, file)
            
            return output_path
            
    except Exception as e:
        print(f"YouTube ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def split_audio_by_size(audio_path, max_size_mb=24):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    
    Args:
        audio_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        max_size_mb (int): ìµœëŒ€ íŒŒì¼ í¬ê¸° (MB)
    
    Returns:
        list: ë¶„í• ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ëª©ë¡
    """
    try:
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
        
        if file_size_mb <= max_size_mb:
            return [audio_path]
        
        # pydubìœ¼ë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = AudioSegment.from_wav(audio_path)
        
        # ì²­í¬ ê¸¸ì´ ê³„ì‚° (íŒŒì¼ í¬ê¸° ê¸°ì¤€)
        total_duration = len(audio)  # ë°€ë¦¬ì´ˆ
        chunk_duration = int((max_size_mb / file_size_mb) * total_duration * 0.9)  # ì•ˆì „ ë§ˆì§„
        
        chunks = []
        temp_dir = os.path.dirname(audio_path)
        base_name = os.path.splitext(os.path.basename(audio_path))[0]
        
        # ì˜¤ë””ì˜¤ ë¶„í• 
        for i, start_time in enumerate(range(0, total_duration, chunk_duration)):
            end_time = min(start_time + chunk_duration, total_duration)
            chunk = audio[start_time:end_time]
            
            chunk_path = os.path.join(temp_dir, f"{base_name}_chunk_{i+1}.wav")
            chunk.export(chunk_path, format="wav")
            chunks.append(chunk_path)
        
        return chunks
        
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return [audio_path]

def transcribe_audio(audio_file_path, api_key=None):
    """
    Groq Whisper Turboë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    í° íŒŒì¼ì€ ìë™ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
    
    Args:
        audio_file_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        api_key (str): Groq API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
    
    Returns:
        tuple: (ë³€í™˜ëœ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
    """
    # API í‚¤ ì„¤ì •
    if not api_key:
        api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        raise ValueError("GROQ_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê³µí•˜ì„¸ìš”.")
    
    # Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = Groq(api_key=api_key)
    
    try:
        # íŒŒì¼ í¬ê¸° í™•ì¸ ë° í•„ìš”ì‹œ ë¶„í• 
        audio_chunks = split_audio_by_size(audio_file_path)
        all_transcriptions = []
        all_metadata = []
        
        for i, chunk_path in enumerate(audio_chunks):
            print(f"ì²­í¬ {i+1}/{len(audio_chunks)} ì²˜ë¦¬ ì¤‘...")
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì—´ê¸° ë° ë³€í™˜
            with open(chunk_path, "rb") as audio_file:
                response = client.audio.transcriptions.create(
                    file=audio_file,
                    model="whisper-large-v3-turbo",
                    language="ko",  # í•œêµ­ì–´ ì„¤ì • (ìë™ ê°ì§€í•˜ë ¤ë©´ ì œê±°)
                    response_format="verbose_json"  # ë©”íƒ€ë°ì´í„° í¬í•¨
                )
                
                # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬
                transcription_text = response.text
                print(f"ì²­í¬ {i+1} ì „ì‚¬ ê²°ê³¼: {len(transcription_text)} ë¬¸ì")
                print(f"ì²­í¬ {i+1} ë¯¸ë¦¬ë³´ê¸°: {transcription_text[:100]}...")
                all_transcriptions.append(transcription_text)
                
                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                metadata = {
                    "chunk_number": i + 1,
                    "total_chunks": len(audio_chunks),
                    "language": getattr(response, 'language', 'ko'),
                    "duration": getattr(response, 'duration', None),
                    "segments": getattr(response, 'segments', []),
                    "words": getattr(response, 'words', []) if hasattr(response, 'words') else []
                }
                all_metadata.append(metadata)
            
            # ì„ì‹œ ì²­í¬ íŒŒì¼ ì‚­ì œ (ì›ë³¸ì´ ì•„ë‹Œ ê²½ìš°)
            if chunk_path != audio_file_path:
                try:
                    os.remove(chunk_path)
                except:
                    pass
        
        # ëª¨ë“  ì „ì‚¬ ê²°ê³¼ í•©ì¹˜ê¸°
        combined_text = " ".join(all_transcriptions)
        print(f"ì „ì‚¬ ì™„ë£Œ: ì´ {len(combined_text)} ë¬¸ì")
        print(f"ì „ì‚¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {combined_text[:100]}...")
        return combined_text, all_metadata
        
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

def format_timestamp(seconds):
    """
    ì´ˆë¥¼ HH:mm:ss.SSS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        seconds (float): ì´ˆ ë‹¨ìœ„ ì‹œê°„
    
    Returns:
        str: HH:mm:ss.SSS í˜•ì‹ì˜ ì‹œê°„
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    milliseconds = int((secs % 1) * 1000)
    secs = int(secs)
    
    return f"{hours:02d}:{minutes:02d}:{secs:02d}.{milliseconds:03d}"

def extract_timestamps_and_text(metadata):
    """
    ë©”íƒ€ë°ì´í„°ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    
    Args:
        metadata (list): ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        list: [(ì‹œì‘ì‹œê°„, ëì‹œê°„, í…ìŠ¤íŠ¸), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    """
    timestamps_data = []
    
    for chunk in metadata:
        segments = chunk.get('segments', [])
        for segment in segments:
            start_time = format_timestamp(segment.get('start', 0))
            end_time = format_timestamp(segment.get('end', 0))
            text = segment.get('text', '').strip()
            
            if text:  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                timestamps_data.append((start_time, end_time, text))
    
    return timestamps_data

def parse_timestamp(timestamp_str):
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
    
    Args:
        timestamp_str (str): HH:mm:ss.SSS í˜•ì‹ì˜ íƒ€ì„ìŠ¤íƒ¬í”„
    
    Returns:
        float: ì´ˆ ë‹¨ìœ„ ì‹œê°„
    """
    try:
        parts = timestamp_str.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = float(parts[2])
        return hours * 3600 + minutes * 60 + seconds
    except:
        return 0.0

def fix_common_speech_errors(text):
    """
    ì¼ë°˜ì ì¸ ìŒì„±ì¸ì‹ ì˜¤ë¥˜ íŒ¨í„´ì„ ë³´ì •
    
    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
    
    Returns:
        str: ë³´ì •ëœ í…ìŠ¤íŠ¸
    """
    # ì¼ë°˜ì ì¸ ìŒì„±ì¸ì‹ ì˜¤ë¥˜ íŒ¨í„´ ì‚¬ì „
    corrections = {
        # ê¸°ìˆ  ìš©ì–´
        'PAST API': 'FastAPI',
        'past API': 'FastAPI',
        'íŒ¨ìŠ¤íŠ¸ API': 'FastAPI',
        'ë³´ì»¤': 'ë„ì»¤',
        'Raspberry': 'ë¼ì¦ˆë² ë¦¬íŒŒì´',
        '3í¬ 1ë¨': '3B 1GB',
        'ì†”ë¡¬ë´‡': 'ì†”ë¡ ë´‡',
        'ì œë¯¸ë‚˜ì´': 'Gemini',
        
        # ì¼ë°˜ ë‹¨ì–´
        'ì›… ë– ë²„ë¦°': 'ë¹„ì–´ë²„ë¦°',
        'ì„¸ì¡°ë¦½': 'ì„¸ ì¤„',
        'ë¹µê¾¸ í„°ì§„': 'ë»¥ ëš«ë¦°',
        'ê³µí°': 'í°',
        'ë¹ˆí°': 'í°',
        'ëƒ…ë‹ˆë‹¤': 'ë‚˜ì˜µë‹ˆë‹¤',
        'ë‚œì¡í•˜ê²Œ': 'ì´ë ‡ê²Œ',
        'ë©”ì‹ ì € ë³´ë“œ': 'ë©”ì‹ ì € ë´‡',
        
        # ë¬¸ë²•/ë§ì¶¤ë²•
        'ê·¸ë‹ˆê¹Œ': 'ê·¸ëŸ¬ë‹ˆê¹Œ',
        'ê°€ì§€ê³ ': 'ê°€ì§€ê³ ',
        'ê±¸ê»ë‹ˆë‹¤': 'ê²ë‹ˆë‹¤',
        'ë˜ê² ì£ ': 'ë˜ê² ì£ ',
        'í•˜ê±°ë“ ìš”': 'í•˜ê±°ë“ ìš”',
    }
    
    corrected_text = text
    for wrong, correct in corrections.items():
        corrected_text = corrected_text.replace(wrong, correct)
    
    return corrected_text

def fix_ai_based_corrections(text, api_key=None):
    """
    AIë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ë° ë³´ì •
    
    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        api_key (str): Groq API í‚¤
    
    Returns:
        str: ë³´ì •ëœ í…ìŠ¤íŠ¸
    """
    if not api_key:
        api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        print("AI ê¸°ë°˜ ë³´ì •ì„ ìœ„í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë³´ì •ë§Œ ì ìš©í•©ë‹ˆë‹¤.")
        return text
    
    try:
        client = Groq(api_key=api_key)
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        max_chunk_length = 1500  # ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        
        print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        if len(text) <= max_chunk_length:
            chunks = [text]
        else:
            # ë” ì„¸ë°€í•œ ë¶„í•  ë°©ë²• ì‚¬ìš©
            # ë¨¼ì € ë¬¸ì¥ìœ¼ë¡œ ë¶„í•  ì‹œë„ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
            sentences = re.split(r'[.!?]\s*', text)
            
            # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì‰¼í‘œë¡œ ë¶„í• 
            if len(sentences) <= 1:
                sentences = re.split(r',\s*', text)
            
            # ê·¸ë˜ë„ ì•ˆë˜ë©´ ê³µë°±ìœ¼ë¡œ ë¶„í• 
            if len(sentences) <= 1:
                sentences = text.split(' ')
            
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ê°€ ì´ˆê³¼ë˜ëŠ”ì§€ í™•ì¸
                test_chunk = current_chunk + (" " if current_chunk else "") + sentence
                
                if len(test_chunk) <= max_chunk_length:
                    current_chunk = test_chunk
                else:
                    # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                    if current_chunk:
                        chunks.append(current_chunk)
                    
                    # ë¬¸ì¥ ìì²´ê°€ ë„ˆë¬´ ê¸¸ë©´ ê°•ì œë¡œ ë¶„í• 
                    if len(sentence) > max_chunk_length:
                        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
                        words = sentence.split(' ')
                        temp_chunk = ""
                        for word in words:
                            if len(temp_chunk + " " + word) <= max_chunk_length:
                                temp_chunk += (" " if temp_chunk else "") + word
                            else:
                                if temp_chunk:
                                    chunks.append(temp_chunk)
                                temp_chunk = word
                        if temp_chunk:
                            current_chunk = temp_chunk
                    else:
                        current_chunk = sentence
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                chunks.append(current_chunk)
        
        print(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")
        
        corrected_chunks = []
        
        for i, chunk in enumerate(chunks):
            print(f"AI ë³´ì • ì¤‘... ({i+1}/{len(chunks)}) - ì²­í¬ ê¸¸ì´: {len(chunk)}")
            
            if not chunk.strip():
                continue
            
            prompt = f"""ë‹¤ìŒì€ í•œêµ­ì–´ ìŒì„±ì¸ì‹ìœ¼ë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
ìŒì„±ì¸ì‹ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”:

1. ì˜ëª» ì¸ì‹ëœ ê¸°ìˆ  ìš©ì–´ë‚˜ ê³ ìœ ëª…ì‚¬ ìˆ˜ì •
2. ë¬¸ë²•ì ìœ¼ë¡œ ì–´ìƒ‰í•œ ë¶€ë¶„ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •
3. ì˜ë¯¸ê°€ í†µí•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë‚˜ êµ¬ë¬¸ ìˆ˜ì •
4. ì „ì²´ì ì¸ ë¬¸ë§¥ê³¼ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
5. ì›ë³¸ì˜ ì˜ë¯¸ì™€ ê¸¸ì´ë¥¼ ìµœëŒ€í•œ ìœ ì§€

ì›ë³¸ í…ìŠ¤íŠ¸:
{chunk}

ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš” (ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì„¤ëª… ì—†ì´):"""

            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ êµì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìŒì„±ì¸ì‹ ì˜¤ë¥˜ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ë˜, ì›ë³¸ì˜ ì˜ë¯¸ì™€ ê¸¸ì´ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=3000  # ë” ë§ì€ í† í° í—ˆìš©
            )
            
            corrected_chunk = response.choices[0].message.content.strip()
            print(f"ì²­í¬ {i+1} ë³´ì • ì™„ë£Œ: {len(corrected_chunk)} ë¬¸ì")
            
            if corrected_chunk:
                corrected_chunks.append(corrected_chunk)
        
        final_result = " ".join(corrected_chunks)
        print(f"ìµœì¢… ë³´ì •ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(final_result)} ë¬¸ì")
        
        return final_result
        
    except Exception as e:
        print(f"AI ê¸°ë°˜ ë³´ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return text

def sort_timestamps_and_fix_overlaps(timestamps_data):
    """
    íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ê²¹ì¹˜ëŠ” ë¶€ë¶„ ìˆ˜ì •
    
    Args:
        timestamps_data (list): [(ì‹œì‘ì‹œê°„, ëì‹œê°„, í…ìŠ¤íŠ¸), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        list: ì •ë ¬ ë° ìˆ˜ì •ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„°
    """
    # ì‹œì‘ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_data = sorted(timestamps_data, key=lambda x: parse_timestamp(x[0]))
    
    # ê²¹ì¹˜ëŠ” ì‹œê°„ êµ¬ê°„ ìˆ˜ì •
    fixed_data = []
    for i, (start, end, text) in enumerate(sorted_data):
        start_seconds = parse_timestamp(start)
        end_seconds = parse_timestamp(end)
        
        # ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
        if fixed_data and start_seconds < parse_timestamp(fixed_data[-1][1]):
            # ê²¹ì¹˜ëŠ” ê²½ìš° ì‹œì‘ ì‹œê°„ì„ ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ë ì‹œê°„ìœ¼ë¡œ ì¡°ì •
            adjusted_start = fixed_data[-1][1]
            # ë ì‹œê°„ì´ ì‹œì‘ ì‹œê°„ë³´ë‹¤ ì‘ìœ¼ë©´ ì ì ˆíˆ ì¡°ì •
            if end_seconds <= parse_timestamp(adjusted_start):
                end_seconds = parse_timestamp(adjusted_start) + 1.0
                adjusted_end = format_timestamp(end_seconds)
            else:
                adjusted_end = end
            
            fixed_data.append((adjusted_start, adjusted_end, text))
        else:
            fixed_data.append((start, end, text))
    
    return fixed_data

def correct_transcription_text(text, metadata, api_key=None, use_ai=True):
    """
    ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë³´ì •
    
    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        metadata (list): ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        api_key (str): Groq API í‚¤
        use_ai (bool): AI ê¸°ë°˜ ë³´ì • ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        tuple: (ë³´ì •ëœ í…ìŠ¤íŠ¸, ë³´ì •ëœ ë©”íƒ€ë°ì´í„°)
    """
    print("í…ìŠ¤íŠ¸ ë³´ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
    print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
    
    # 1. ê¸°ë³¸ íŒ¨í„´ ë³´ì •
    print("1ë‹¨ê³„: ì¼ë°˜ì ì¸ ìŒì„±ì¸ì‹ ì˜¤ë¥˜ ë³´ì •...")
    corrected_text = fix_common_speech_errors(text)
    print(f"1ë‹¨ê³„ ë³´ì • í›„ ê¸¸ì´: {len(corrected_text)} ë¬¸ì")
    
    # 2. AI ê¸°ë°˜ ë³´ì • (ì˜µì…˜)
    if use_ai:
        print("2ë‹¨ê³„: AI ê¸°ë°˜ í…ìŠ¤íŠ¸ ë³´ì •...")
        print(f"ë³´ì • ì „ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(corrected_text)} ë¬¸ì")
        ai_corrected_text = fix_ai_based_corrections(corrected_text, api_key)
        print(f"AI ë³´ì • ê²°ê³¼ ê¸¸ì´: {len(ai_corrected_text) if ai_corrected_text else 0} ë¬¸ì")
        
        # AI ë³´ì • ê²°ê³¼ê°€ ì›ë³¸ì˜ 50% ì´ìƒì´ì–´ì•¼ ì‚¬ìš©
        if ai_corrected_text and len(ai_corrected_text.strip()) > len(corrected_text) * 0.5:
            corrected_text = ai_corrected_text
            print(f"AI ë³´ì • ì™„ë£Œ: {len(corrected_text)} ë¬¸ì")
        else:
            print(f"AI ë³´ì • ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ (AI: {len(ai_corrected_text) if ai_corrected_text else 0}, ì›ë³¸: {len(corrected_text)}) - ê¸°ë³¸ ë³´ì •ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # 3. íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° ë³´ì •
    print("3ë‹¨ê³„: íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬ ë° ë³´ì •...")
    corrected_metadata = []
    
    for chunk in metadata:
        # ê° ì²­í¬ì˜ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
        segments = chunk.get('segments', [])
        timestamps_data = []
        
        for segment in segments:
            start_time = format_timestamp(segment.get('start', 0))
            end_time = format_timestamp(segment.get('end', 0))
            segment_text = segment.get('text', '').strip()
            
            if segment_text:
                # ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë„ ë³´ì •
                corrected_segment_text = fix_common_speech_errors(segment_text)
                timestamps_data.append((start_time, end_time, corrected_segment_text))
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬ ë° ê²¹ì¹¨ ìˆ˜ì •
        fixed_timestamps = sort_timestamps_and_fix_overlaps(timestamps_data)
        
        # ìˆ˜ì •ëœ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        updated_segments = []
        for start_time, end_time, segment_text in fixed_timestamps:
            updated_segments.append({
                'start': parse_timestamp(start_time),
                'end': parse_timestamp(end_time),
                'text': segment_text
            })
        
        corrected_chunk = chunk.copy()
        corrected_chunk['segments'] = updated_segments
        corrected_metadata.append(corrected_chunk)
    
    print("í…ìŠ¤íŠ¸ ë³´ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return corrected_text, corrected_metadata

def save_subtitle_and_metadata(text, metadata, output_dir, base_filename, format="txt", correction_enabled=True, use_ai_correction=True):
    """
    ìë§‰ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ë³´ì • ê¸°ëŠ¥ í¬í•¨)
    
    Args:
        text (str): ìë§‰ í…ìŠ¤íŠ¸
        metadata (list): ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
        base_filename (str): ê¸°ë³¸ íŒŒì¼ëª…
        format (str): ì¶œë ¥ í˜•ì‹ (txt, srt)
        correction_enabled (bool): í…ìŠ¤íŠ¸ ë³´ì • ì‚¬ìš© ì—¬ë¶€
        use_ai_correction (bool): AI ê¸°ë°˜ ë³´ì • ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        tuple: (ìë§‰ íŒŒì¼ ê²½ë¡œ, ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ, íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ ê²½ë¡œ, ë³´ì •ëœ íŒŒì¼ë“¤ ê²½ë¡œ)
    """
    import json
    
    try:
        # ì›ë³¸ íŒŒì¼ë“¤ ë¨¼ì € ì €ì¥
        subtitle_path = os.path.join(output_dir, f"{base_filename}.{format}")
        metadata_path = os.path.join(output_dir, f"{base_filename}_metadata.json")
        timestamp_path = os.path.join(output_dir, f"{base_filename}_timestamps.txt")
        
        # ì›ë³¸ ìë§‰ ì €ì¥
        if format == "txt":
            with open(subtitle_path, 'w', encoding='utf-8') as f:
                f.write(text)
        elif format == "srt":
            with open(subtitle_path, 'w', encoding='utf-8') as f:
                f.write("1\n")
                f.write("00:00:00,000 --> 99:59:59,999\n")
                f.write(text)
                f.write("\n")
        
        # ì›ë³¸ ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        # ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
        timestamps_data = extract_timestamps_and_text(metadata)
        with open(timestamp_path, 'w', encoding='utf-8') as f:
            f.write("íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ìë§‰\n")
            f.write("=" * 50 + "\n\n")
            for start_time, end_time, segment_text in timestamps_data:
                f.write(f"[{start_time} â†’ {end_time}] {segment_text}\n")
        
        print(f"ì›ë³¸ ìë§‰ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {subtitle_path}")
        print(f"ì›ë³¸ ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {metadata_path}")
        print(f"ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {timestamp_path}")
        
        corrected_files = None
        
        # ë³´ì • ê¸°ëŠ¥ì´ í™œì„±í™”ëœ ê²½ìš°
        if correction_enabled:
            try:
                # í…ìŠ¤íŠ¸ ë³´ì •
                corrected_text, corrected_metadata = correct_transcription_text(
                    text, metadata, use_ai=use_ai_correction
                )
                
                # ë³´ì •ëœ íŒŒì¼ë“¤ ì €ì¥
                corrected_subtitle_path = os.path.join(output_dir, f"{base_filename}_corrected.{format}")
                corrected_metadata_path = os.path.join(output_dir, f"{base_filename}_corrected_metadata.json")
                corrected_timestamp_path = os.path.join(output_dir, f"{base_filename}_corrected_timestamps.txt")
                
                # ë³´ì •ëœ ìë§‰ ì €ì¥
                if format == "txt":
                    with open(corrected_subtitle_path, 'w', encoding='utf-8') as f:
                        f.write(corrected_text)
                elif format == "srt":
                    with open(corrected_subtitle_path, 'w', encoding='utf-8') as f:
                        f.write("1\n")
                        f.write("00:00:00,000 --> 99:59:59,999\n")
                        f.write(corrected_text)
                        f.write("\n")
                
                # ë³´ì •ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
                with open(corrected_metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(corrected_metadata, f, indent=2, ensure_ascii=False)
                
                # ë³´ì •ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
                corrected_timestamps_data = extract_timestamps_and_text(corrected_metadata)
                with open(corrected_timestamp_path, 'w', encoding='utf-8') as f:
                    f.write("íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ìë§‰ (ë³´ì •ë¨)\n")
                    f.write("=" * 50 + "\n\n")
                    for start_time, end_time, corrected_segment_text in corrected_timestamps_data:
                        f.write(f"[{start_time} â†’ {end_time}] {corrected_segment_text}\n")
                
                corrected_files = (corrected_subtitle_path, corrected_metadata_path, corrected_timestamp_path)
                
                print(f"ë³´ì •ëœ ìë§‰ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {corrected_subtitle_path}")
                print(f"ë³´ì •ëœ ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {corrected_metadata_path}")
                print(f"ë³´ì •ëœ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {corrected_timestamp_path}")
                
            except Exception as e:
                print(f"í…ìŠ¤íŠ¸ ë³´ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ì›ë³¸ íŒŒì¼ë§Œ ì €ì¥ë©ë‹ˆë‹¤.")
        
        return subtitle_path, metadata_path, timestamp_path, corrected_files
        
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None, None

def create_output_directory(base_dir="./result", video_title=None):
    """
    yyyyMMDD_HHmmss_íƒ€ì´í‹€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    
    Args:
        base_dir (str): ê¸°ë³¸ ë””ë ‰í† ë¦¬
        video_title (str): ë¹„ë””ì˜¤ ì œëª©
    
    Returns:
        str: ìƒì„±ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ í´ë”ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë¹„ë””ì˜¤ ì œëª©ì´ ìˆìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
    if video_title:
        safe_title = re.sub(r'[<>:"/\\|?*]', '', video_title)[:50]  # ìµœëŒ€ 50ìë¡œ ì œí•œ
        folder_name = f"{timestamp}_{safe_title}"
    else:
        folder_name = timestamp
    
    output_dir = os.path.join(base_dir, folder_name)
    os.makedirs(output_dir, exist_ok=True)
    
    return output_dir

def youtube_to_subtitle(url, base_output_dir="./result", subtitle_format="txt", enable_correction=True, use_ai_correction=True):
    """
    YouTube ë§í¬ì—ì„œ ìë§‰ ìƒì„± (ë³´ì • ê¸°ëŠ¥ í¬í•¨)
    
    Args:
        url (str): YouTube URL
        base_output_dir (str): ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬
        subtitle_format (str): ìë§‰ í˜•ì‹ (txt, srt)
        enable_correction (bool): í…ìŠ¤íŠ¸ ë³´ì • ì‚¬ìš© ì—¬ë¶€
        use_ai_correction (bool): AI ê¸°ë°˜ ë³´ì • ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        tuple: (ì›ë³¸ íŒŒì¼ë“¤, ë³´ì •ëœ íŒŒì¼ë“¤, ì¶œë ¥ ë””ë ‰í† ë¦¬)
    """
    try:
        # ë¨¼ì € ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ì œëª© í™•ì¸
        print("YouTube ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        ydl_opts = {'quiet': True}
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            video_title = info.get('title', 'unknown')
        
        # ë‚ ì§œ/ì‹œê°„/ì œëª© í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = create_output_directory(base_output_dir, video_title)
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")
        
        print("YouTubeì—ì„œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        audio_path = download_youtube_audio(url)
        
        if not audio_path:
            print("ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None, None
        
        print(f"ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {audio_path}")
        print("Groq Whisper Turboë¡œ ìë§‰ ìƒì„± ì¤‘...")
        
        # ìŒì„± ì¸ì‹ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        subtitle_text, metadata = transcribe_audio(audio_path)
        
        if not subtitle_text:
            print("ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None, None
        
        # ìë§‰ ë° ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥ (ë³´ì • í¬í•¨)
        video_name = Path(audio_path).stem
        result = save_subtitle_and_metadata(
            subtitle_text, metadata, output_dir, video_name, subtitle_format,
            correction_enabled=enable_correction, use_ai_correction=use_ai_correction
        )
        
        # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(audio_path)
            temp_dir = os.path.dirname(audio_path)
            if temp_dir.startswith(tempfile.gettempdir()):
                os.rmdir(temp_dir)
        except:
            pass
        
        if result and len(result) == 4:
            original_files = result[:3]
            corrected_files = result[3]
            return original_files, corrected_files, output_dir
        else:
            return result[:3] if result else None, None, output_dir
        
    except Exception as e:
        print(f"ìë§‰ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None

def correct_existing_transcript_file(input_file_path, output_file_path=None, use_ai_correction=True):
    """
    ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ì„ ë³´ì •
    
    Args:
        input_file_path (str): ì…ë ¥ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ ê²½ë¡œ
        output_file_path (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ _corrected ì ‘ë¯¸ì‚¬ ì¶”ê°€)
        use_ai_correction (bool): AI ê¸°ë°˜ ë³´ì • ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        str: ë³´ì •ëœ íŒŒì¼ ê²½ë¡œ
    """
    try:
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        if not output_file_path:
            base_path = os.path.splitext(input_file_path)[0]
            ext = os.path.splitext(input_file_path)[1]
            output_file_path = f"{base_path}_corrected{ext}"
        
        print(f"íŒŒì¼ ë³´ì • ì‹œì‘: {input_file_path}")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ ì½ê¸°
        with open(input_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # í—¤ë” ë¼ì¸ ì°¾ê¸°
        content_start_idx = 0
        for i, line in enumerate(lines):
            if line.strip().startswith('[') and ' â†’ ' in line:
                content_start_idx = i
                break
        
        header_lines = lines[:content_start_idx] if content_start_idx > 0 else ["íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ìë§‰ (ë³´ì •ë¨)\n", "=" * 50 + "\n\n"]
        timestamp_lines = lines[content_start_idx:]
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° íŒŒì‹±
        timestamps_data = []
        for line in timestamp_lines:
            line = line.strip()
            if line and line.startswith('[') and ' â†’ ' in line:
                # [ì‹œì‘ì‹œê°„ â†’ ëì‹œê°„] í…ìŠ¤íŠ¸ í˜•ì‹ íŒŒì‹±
                match = re.match(r'\[(.+?) â†’ (.+?)\] (.+)', line)
                if match:
                    start_time, end_time, text = match.groups()
                    timestamps_data.append((start_time.strip(), end_time.strip(), text.strip()))
        
        print(f"ì´ {len(timestamps_data)}ê°œì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # 1. ê¸°ë³¸ íŒ¨í„´ ë³´ì •
        print("1ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ ë³´ì •...")
        corrected_timestamps = []
        for start_time, end_time, text in timestamps_data:
            corrected_text = fix_common_speech_errors(text)
            corrected_timestamps.append((start_time, end_time, corrected_text))
        
        # 2. íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬ ë° ê²¹ì¹¨ ìˆ˜ì •
        print("2ë‹¨ê³„: íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬...")
        sorted_timestamps = sort_timestamps_and_fix_overlaps(corrected_timestamps)
        
        # 3. AI ê¸°ë°˜ í…ìŠ¤íŠ¸ ë³´ì • (ì˜µì…˜)
        if use_ai_correction:
            print("3ë‹¨ê³„: AI ê¸°ë°˜ í…ìŠ¤íŠ¸ ë³´ì •...")
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = " ".join([text for _, _, text in sorted_timestamps])
            corrected_full_text = fix_ai_based_corrections(full_text)
            
            # AI ë³´ì •ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í•  (ê°„ë‹¨í•œ ë°©ë²•)
            # ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ì™€ ë™ì¼í•˜ê²Œ ë¶„í•  ì‹œë„
            corrected_sentences = re.split(r'[.!?]\s+', corrected_full_text)
            
            if len(corrected_sentences) >= len(sorted_timestamps):
                # AI ë³´ì • ê²°ê³¼ë¥¼ ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ì— ë§¤í•‘
                final_timestamps = []
                sentence_idx = 0
                
                for start_time, end_time, original_text in sorted_timestamps:
                    if sentence_idx < len(corrected_sentences):
                        corrected_text = corrected_sentences[sentence_idx].strip()
                        if not corrected_text:
                            sentence_idx += 1
                            if sentence_idx < len(corrected_sentences):
                                corrected_text = corrected_sentences[sentence_idx].strip()
                        
                        final_timestamps.append((start_time, end_time, corrected_text))
                        sentence_idx += 1
                    else:
                        final_timestamps.append((start_time, end_time, original_text))
                
                sorted_timestamps = final_timestamps
        
        # ë³´ì •ëœ íŒŒì¼ ì €ì¥
        with open(output_file_path, 'w', encoding='utf-8') as f:
            # í—¤ë” ì‘ì„±
            f.write("íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ìë§‰ (ë³´ì •ë¨)\n")
            f.write("=" * 50 + "\n\n")
            
            # ë³´ì •ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° ì‘ì„±
            for start_time, end_time, text in sorted_timestamps:
                f.write(f"[{start_time} â†’ {end_time}] {text}\n")
        
        print(f"ë³´ì •ëœ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file_path}")
        return output_file_path
        
    except Exception as e:
        print(f"íŒŒì¼ ë³´ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    print("YouTube ìë§‰ ìƒì„±ê¸° (í…ìŠ¤íŠ¸ ë³´ì • ê¸°ëŠ¥ í¬í•¨)")
    print("=" * 50)
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ URL ë°›ê¸°
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        # ì‚¬ìš©ìì—ê²Œ URL ì…ë ¥ë°›ê¸°
        url = input("YouTube URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        
    # URLì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
    if not url:
        print("âŒ ì—ëŸ¬: YouTube URLì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì‚¬ìš©ë²•:")
        print("  python main.py <YouTube_URL>")
        print("  ë˜ëŠ” í”„ë¡œê·¸ë¨ ì‹¤í–‰ í›„ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    # ìë§‰ í˜•ì‹ (ê¸°ë³¸ê°’: txt)
    subtitle_format = "txt"
    
    # ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì‹¤ì œ ì¶œë ¥ì€ ë‚ ì§œ/ì‹œê°„/ì œëª© í´ë”ë¡œ ìƒì„±ë¨)
    base_output_dir = "./result"
    
    # ë³´ì • ê¸°ëŠ¥ ì„¤ì •
    enable_correction = True
    use_ai_correction = True
    
    print(f"\nìë§‰ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"URL: {url}")
    print(f"í˜•ì‹: {subtitle_format.upper()}")
    print(f"ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: {base_output_dir}")
    print(f"í…ìŠ¤íŠ¸ ë³´ì •: {'í™œì„±í™”' if enable_correction else 'ë¹„í™œì„±í™”'}")
    print(f"AI ë³´ì •: {'í™œì„±í™”' if use_ai_correction else 'ë¹„í™œì„±í™”'}")
    print("-" * 50)
    
    try:
        # ìë§‰ ìƒì„± (ë³´ì • ê¸°ëŠ¥ í¬í•¨)
        original_files, corrected_files, actual_output_dir = youtube_to_subtitle(
            url, base_output_dir, subtitle_format, enable_correction, use_ai_correction
        )
        
        print(f"\nğŸ“ ì‹¤ì œ ì¶œë ¥ ë””ë ‰í† ë¦¬: {actual_output_dir}")
        
        if original_files:
            subtitle_path, metadata_path, timestamp_path = original_files
            print("\nâœ… ìë§‰ ìƒì„± ì™„ë£Œ!")
            print(f"ì›ë³¸ ìë§‰ íŒŒì¼: {subtitle_path}")
            print(f"ì›ë³¸ ë©”íƒ€ë°ì´í„° íŒŒì¼: {metadata_path}")
            print(f"ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼: {timestamp_path}")
            
            if corrected_files:
                corrected_subtitle_path, corrected_metadata_path, corrected_timestamp_path = corrected_files
                print(f"\nğŸ“ ë³´ì •ëœ ìë§‰ íŒŒì¼: {corrected_subtitle_path}")
                print(f"ğŸ“ ë³´ì •ëœ ë©”íƒ€ë°ì´í„° íŒŒì¼: {corrected_metadata_path}")
                print(f"ğŸ“ ë³´ì •ëœ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼: {corrected_timestamp_path}")
            
            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            try:
                with open(subtitle_path, 'r', encoding='utf-8') as f:
                    content = f.read()[:500]  # ì²˜ìŒ 500ìë§Œ í‘œì‹œ
                    print("\n=== ì›ë³¸ ìë§‰ ë¯¸ë¦¬ë³´ê¸° ===")
                    print(content)
                    if len(content) == 500:
                        print("...")
                
                if corrected_files:
                    with open(corrected_subtitle_path, 'r', encoding='utf-8') as f:
                        corrected_content = f.read()[:500]
                        print("\n=== ë³´ì •ëœ ìë§‰ ë¯¸ë¦¬ë³´ê¸° ===")
                        print(corrected_content)
                        if len(corrected_content) == 500:
                            print("...")
                
                # ë©”íƒ€ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                import json
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    print(f"\n=== ë©”íƒ€ë°ì´í„° ì •ë³´ ===")
                    print(f"ì´ ì²­í¬ ìˆ˜: {len(metadata)}")
                    if metadata:
                        first_chunk = metadata[0]
                        print(f"ì–¸ì–´: {first_chunk.get('language', 'N/A')}")
                        print(f"ì´ ì§€ì†ì‹œê°„: {first_chunk.get('duration', 'N/A')}ì´ˆ")
                        if first_chunk.get('segments'):
                            print(f"ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(first_chunk['segments'])}")
            except:
                pass
        else:
            print("âŒ ìë§‰ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()